filebeat.inputs:
- type: log #Change value to true to activate the input configuration
  enabled: false
  paths:
    - /var/log/apache2/*
    - /var/log/nginx/*
    - /var/log/mysql/*
- type: docker
  combine_partial: true
  containers:
    path: "/usr/share/dockerlogs/data"
    stream: "stdout"
    ids:
      - "*"
  exclude_files: ['\.gz$']
  ignore_older: 10m

filebeat.autodiscover:
 providers:
   - type: docker
     templates:
       - condition:
         contains:
           docker.container.image: nginx
         config:
           - module: nginx
             access:
               input:
                 type: docker
                 containers.ids:
                   - "${data.docker.container.id}"
             error:
               input:
                 type: docker
                 containers.ids:
                   - "${data.docker.container.id}"

processors:
  # decode the log field (sub JSON document) if JSON encoded, then maps it's fields to elasticsearch fields
- decode_json_fields:
    fields: ["log", "message"]
    target: ""
    # overwrite existing target elasticsearch fields while decoding json fields    
    overwrite_keys: true
- add_docker_metadata:
    host: "unix:///var/run/docker.sock"

filebeat.config.modules:
  path: ${path.config}/modules.d/*.yml
  eload.period: 10s
  reload.enabled: true

output.elasticsearch:
  hosts: ["${ELK_ELASTICSEARCH_HOST}"]
  username: elastic
  password: ${ELK_ELASTICSEARCH_PASSWORD}

# # setup filebeat to send output to logstash
# output.logstash:
#   hosts: ["logstash"]

# Write Filebeat own logs only to file to avoid catching them with itself in docker log files
logging.level: error
logging.to_files: false
logging.to_syslog: false
loggins.metrice.enabled: false
logging.files:
  path: /var/log/filebeat
  name: filebeat
  keepfiles: 7
  permissions: 0644
ssl.verification_mode: none


setup.kibana:
  host: "http://${ELK_KIBANA_HOST}"
  username: elastic
  password: ${ELK_ELASTICSEARCH_PASSWORD}

setup.dashboards.enabled: true